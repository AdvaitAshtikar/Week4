---
title: "Weekly Summary Template"
author: "Advait Ashtikar"
title-block-banner: true
title-block-style: default
toc: true
# format: html
format: pdf
---

------------------------------------------------------------------------

## Tuesday, Jan 30

::: callout-important
## TIL

Include a *very brief* summary of what you learnt in this class here.

Today, I learnt the following concepts in class:

1.  Intro to Statistical Learning
2.  Simple Linear Regression
    1.  Motivation

    2.  $\ell_{2}$ estimator

    3.  Inference

    4.  Prediction
:::

## Loading Libraries

```{r}
library(tidyverse)
library(ISLR2)
library(cowplot)
library(kableExtra)
library(htmlwidgets)
```

## Statistical Learning

Suppose we have a data set

**X = \[**$X_{1}$ , $X_{2}$, .... $X_{n}$**\]**

-   These are called predictor/independent variables

**Y**

-   Th

The goal of statistical learning is to find a function $f$ such that $y$ = $f(x)$

### Different flavors: Statistical learning

-   Supervised learning (Both y and x)

    -   Regression

    -   Classification

-   Unsupervised learning (There is no y; much harder)

-   Semi-supervised learning (The case when you have y but x is something else)

-   Reinforcement learning (Corresponds to a case where the model is thought to do the work)

```{r}
## URL for the dataset:
url <- "https://online.stat.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/data/poverty/index.txt"

df <- read_tsv(url)
df %>% head(., 10) %>% knitr::kable()
```

### Goal

Prdict the birth rate as a function of the poverty rate

```{r}
colnames(df) <- tolower(colnames(df))
x <- df$povpct
y <- df$brth15to17
```

#### Scatterplot

Visualize the relationship between the $x$ and $y$ variables

```{r}
plt <- function(){
 plot(
  x,
  y,
  pch=20,
  xlab = "Pov %",
  ylab = "Birth rate (15 - 17)"
) 
}
plt()
```

#### Lines through the points

```{r}
b0 <- c(-2, 0, 2)
b1 <- c(0, 1, 2)

par(mfrow=c(3, 3))

for(B0 in b0){
  for(B1 in b1){
    plt()
    curve( B0 + B1 * x, 0, 30, add=T, col="red")
    title(main = paste("b0 = ", B0," and b1 = ",B1))
  }
}
```

## Least squares estimator

```{r}
b0 <- 10
b1 <- 1

yhat <- b0 + b1 * x

plt()
curve( B0 + B1 * x, 0, 30, add=T, col="red")
title(main = paste("b0 = ", B0," and b1 = ",B1))
segments(x, y, x, yhat)

resids <- abs(y - yhat)^2
ss_resids <- sum(resids)
title(main = paste("bo, b1, ss_residuals = ", b0, b1, ss_resids, sep = ","))
```

The best fit line minimizes residuals

```{r}
model <- lm(y ~ x)
sum(residuals(model)^2)
```

```{r}
summary(model)
```

The summary for the model contains the optimal slope.

## Thursday, Jan 19

::: callout-important
## TIL

Include a *very brief* summary of what you learnt in this class here.

Today, I learnt the following concepts in class:

1.  Linear Regression
2.  Multiple Regression
    1.  Extension from simple linear regression
:::

## Model Formulae

In our case we want to model $y$ as a function of $x$. In 'R' the formula for this looks like:

```{r}
typeof(formula(y~x))
```

A linear regression model in 'R' is called using the **L**inear **M**odel, i.e., 'lm()'

```{r}
model <- lm(y~x)
```

**Q.** What are the null and alternate hypotheses for a regression model?

Objective: We want to find the best linear model to fit $y \sin x$

Null Hypotheses: There is no linear relationship between $y$ and $x$.

-   What does this mean in terms of $\beta_0$ and $\beta_1$

Alternate Hypotheses: $\beta_1 \neq 0$

**To summarize**

$$
\begin{align}
H_0: \beta_1 = 0 && H_1: \beta_1 \neq 0
\end{align}
$$

When we see a small $p$-value, then we reject the null hypothesis in favor of the alternate hypothesis. What is the implication of this w.r.t. the original model objective?

> \*\* There is a significant relationship between $y$ and $x$. Or, in more mathematical terms, there is significant evidence in favor of a correlation between $x$ and $y$ \*\*

This is what the $p$-value in the model output are capturing. We can also use the 'kable' function to print the results nicely:

```{r}
library(broom)

summary(model) %>%
  broom::tidy() %>%
  knitr::kable()
```

## Regression Models

1.  Independent variable $x$

```{r}
head(x)
```

2.  Response $y$

```{r}
head(y)
```

3.  Fitted values $\hat{y}$

```{r}
yhat <- fitted(model)
head(yhat)
```

4.  Residuals: $e = y - \hat{y}$

```{r}
res <- residuals(model)
head(res)
```

Some other important terms are the following:

1.  Sum of squares for residuals:

$SS_{Res} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i^2)$

2.  Sum of squares for regression:

$SS_{Reg} = \sum_{i=1}^n (\hat{y}_i - \bar{y})^2$

3.  Sum of squares total:

$SS_{Tot} = \sum_{i=1}^n ({y}_i - \bar{y})^2$

Another important summary in the model output is the $R^2$ value, which is given as follow:

$$
R^2 = \frac{SS_{Reg}}{SS_{Tot}}
$$

Lets have a look at what this means in the following example.

```{r}
x <- seq(0, 5, length=100)

b0 <- 1
b1 <- 3

y1 <- b0 + b1 * x + rnorm(100)
y2 <- b0 + b1 * x + rnorm(100) * 3

par(mfrow=c(1,2))

plot(x, y1)
plot(x, y2)
```

```{r}
model1 <- lm(y1 ~ x)
model2 <- lm(y2 ~ x)

par(mfrow=c(1,2))

plot(x, y1)
curve(
  coef(model1)[1] + coef(model1)[2] * x,
  add=T, col="red"
)

plot(x, y2)
curve(
  coef(model2)[1] + coef(model2)[2] * x,
  add=T, col="red"
)
```

The summary of model 1 is:

```{r}
summary(model1)
```

The summary for model2:

```{r}
summary(model2)
```

The last thing we're going to talk about in simple linear regression is **prediction**. It's the ability of a model to predict values for "unseen" data.

Let's go back to the poverty dataset.

```{r}
x <- df$povpct
y <- df$brth15to17
plt()
```

Suppose we have a "new" state formed whose 'povpct' value is $22$.

```{r}
plt()
abline(v=21, col="green")
lines(x, fitted(lm(y~x)), col="red")
```

**Q.** What is the best guess for this prediction going to be? We could consider the graph and our best prediction is going to be the intersection. In $R$, we can use the `predict()` function to do this:

```{r}
new_x <- data.frame(x = c(21))
new_y <- predict(model, new_x)

new_y
```

If we plot this new point we get

```{r}
plt()
abline(v=21, col="green")
lines(x, fitted(lm(y~x)), col="red")
points(new_x, new_y, col="purple")
```

We can make predictions not just for a single observation, but for a whole collection of observations.

```{r}
new_x <- data.frame(x = c(1:21))
new_y <- predict(model, new_x)
new_y
```

This is what the plot looks like:

```{r}
plt()
for(a in new_x){abline(v=a, col="green")}
lines(x, fitted(lm(y~x)), col="red")
points(a, new_y, col="purple")
```
